var documenterSearchIndex = {"docs":
[{"location":"usage/#*Values*","page":"Usage","title":"Values","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's start with the Value composite type. Here's how you define a Value","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SimpleGrad\n\nx = Value(4.0)\n\nprintln(x)\n# output: Value(4.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Values can store numbers, perform operations, and automatically track the gradients of the variables involved.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here's how you take a look at the number a Value is storing (called Value.data), and it's gradient (called Value.grad):","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.data) # the number\n# output: 4.0\n\nprintln(x.grad) # the gradient\n# output: 0.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here, x.data == 4.0 because the Value x is storing the number 4.0, and x.grad == 0.0 is a placeholder for the gradient, which could eventually change if we do some operations and eventually back-calculate the gradient.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next let's try an operation. We'll define another Value called y, add it to x, and save the result as z.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"y = Value(3.0)\nz = x + y\n\nprintln(z)\n# output: Value(7.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Pretty simple so far, right? But here's the cool part – we can now do a backward pass to calculate the derivative of z with respect to x and y. Here's how we do that:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now, the grad fields of x and y are populated, and will tell us the derivative of z with respect to each of the inputs x and y.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.grad) # dz/dx = 1, meaning an increase of 1 in x will lead to an increase of 1 in z.\n# output: 1.0\n\nprintln(y.grad) # dz/dy = 1, meaning an increase of 1 in y will lead to an increase of 1 in z.\n# output: 1.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"In mathematical terms, we're considering the equation z = x + y and are interested in the derivatives fracdzdx and fracdzdy. x.grad == 1 tells us that fracdzdx = 1 and y.grad == 1 tells us that fracdzdy = 1 for the values of x and y that we've defined in our code (and in this specific example, for all values of x and y). If you're rusty on the calculus, you can also think of it this way: increasing x by 1 will cause z to increase by 1, and increasing y by 1 will also cause z to increase by `.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"So that's the basic functionality of the Value class. You can store store numbers, do operations, and track the derivative of the output with respect to all of the inputs. This allows you to, for example, minimize a loss function through gradient-descent, by tracking the derivative of the loss with respect to the model parameters, and then updating those parameters so that the loss decreases.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here's a list of the operations currently supported:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Addition\nSubtraction\nMultiplication\nDivision\nExponents\ne^x\nlog()\ntanh()","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's test a couple of them out. We've already done addition, so let's try subtraction.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(10.0)\ny = Value(3.0)\nz = x - y\n\nprintln(z)\n# output: Value(7.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If you want, you can try backward(z), and you should be able to find x.grad == 1 meaning that  fracdzdx = 1, and x.grad == -1 meaning that fracdzdy = -1. But I'll skip over that for now.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next let's try multiplication.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(6.0)\ny = Value(2.0)\nz = x * y\n\nprintln(z)\n# output: Value(12.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And again, we can get the derivative with of z with respect to x and y.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = y = 2\n# output: 2.0\n\nprintln(y.grad) # dz/dy = x = 6\n# output: 6.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Alright, so far so good! Let's try division now:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(15.0)\ny = Value(5.0)\nz = x / y\n\nprintln(z)\n# output: Value(3.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 1/5 = 0.2\n# output: 0.2\n\nprintln(y.grad) # dz/dy = -15 / x^2 = -0.6\n# output: -0.6","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now let's try exponents. NOTE: for this function, the exponents here must be a regular number, NOT a Value. Might work on fixing this later.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(5.0)\ny = 2 # NOTE - exponent can't be Value, must be int or float.\nz = x^y\n\nprintln(z)\n# output: Value(25.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 2x = 10\n# output: 10.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now for the exponential function e^x, which we will call exp().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(2.0)\nz = exp(x)\n\nprintln(z)\n# output: Value(7.38905609893065)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = e^x = (same thing we got for above)\n# output: 7.38905609893065","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now for the natural logarithm, which we call log().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(10.0)\nz = log(x)\n\nprintln(z)\n# output: Value(2.302585092994046)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 1/x = 0.1\n# output: 0.1","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Lastly, the tanh() function. Personally my trig is pretty rusty and I don't use this function very often, but I'm including it because it was in Andrej Karpathy's Micrograd, which the SimpleGrad Value is based on. tanh()  is useful as a possible activation function for a linear layer of neurons, to add nonlinearity and bound the layer outputs on [-1, 1].","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(3.0)\nz = tanh(x)\n\nprintln(z)\n# output: Value(0.9950547536867305)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\nprintln(x.grad) # dz/dx = 1 - tanh^2(x) = ????\n# output: 0.009866037165440211","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"So far these examples have been pretty simple. But as long as we're using these simple functions, we can combine them in pretty complicated ways. The gradients can still be calculated for all the inputs, using backpropagation and the chain rule of derivatives.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's try out a complicated example to see this...","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"input1 = Value(2.3)\ninput2 = Value(-3.5)\ninput3 = Value(3.9)\n\nweight1 = Value(-0.8)\nweight2 = Value(1.8)\nweight3 = Value(3.0)\n\nbias = Value(-3.2)\n\ny_pred = tanh(input1*weight1 + input2*weight2 + input3*weight3 + bias)\ny_true = Value(0.8)\n\nloss = (y_pred - y_true)^2\n\nprintln(loss)\n# output: Value(0.20683027474728832)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here we're using 3 inputs, 3 weights, a bias, and a tanh() activation function to come up with some prediction in a regression problem, and calculating a loss by comparing it to the target value.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Even though this looks pretty complicated, we can still use backward(loss) to calculate the derivative of the loss with respect to everything.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(loss)\n\nprintln(weight1.grad) # dloss/dweight1\n# output: -1.8427042527651991\n\nprintln(weight2.grad) # dloss/dweight2\n# output: 2.80411516725139\n\nprintln(weight3.grad) # dloss/dweight3\n# output: -3.12458547208012\n\nprintln(bias.grad) # dloss/dbias\n# output: -0.8011757620718257","category":"page"},{"location":"usage/#*Tensors*","page":"Usage","title":"Tensors","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Values are pretty useful for some specific cases, but unfortunately their scalar-valued calculations will be too slow when it comes to implementing even a pretty basic neural network. So in addition to Values, we also have our Tensor composite type, which stores data in array format (either one-dimensional or two-dimensional).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"We can define a Tensor like this:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Tensor([2.0, 3.0, 4.0])\n\nprintln(x)\n# output: Tensor([2.0, 3.0, 4.0])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Similarly to Values, Tensors also have fields called data and grad that store their arrays of numbers and gradients.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.data)\n# output: [2.0, 3.0, 4.0]\n\nprintln(x.grad)\n# output: [0.0, 0.0, 0.0]","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Right now the Tensor class pretty much has the bare minimum needed to implement a simple neural network, although I'm probably going to add more in the future. Here's a list of the operations currently supported:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Addition\nMatrix Multiplication\nReLU\nSoftmax Activation / Cross Entropy Loss Combination","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Rather than testing out all of these individually, let's see if we can save some time by testing them all out at once:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using Random\nRandom.seed!(1234)\n\ninputs = Tensor(rand(2, 3)) # Matrix with shape (2,3) -- 2 batches, 3 input features per batch\nweights1 = Tensor(rand(3, 4)) # Matrix with shape (3,4) -- takes 3 inputs, has 4 neurons\nweights2 = Tensor(rand( 4, 5)) # Matrix with shape (4,5) -- takes 4 inputs, has 5 neurons\nbiases1 = Tensor([1.0,1.0,1.0,1.0]) # Bias vector for first layer neurons\nbiases2 = Tensor([1.0,1.0,1.0,1.0,1.0]) # Bias vector for second layer neurons\n\n\nlayer1_out = relu(inputs * weights1 + biases1)\n\nlayer2_out = layer1_out * weights2 + biases2\n\n\n# important -- correct classes should be one-hot encoded and NOT a Tensor, just a regular matrix.\ny_true = [0 1 0 0 0;\n          0 0 0 1 0]\n\nloss = softmax_crossentropy(layer2_out,y_true)\n\n\n\nprintln(loss)\n# output: Tensor([1.9662258101705288])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now we can find the derivative of the loss with respect to the weights and biases (and inputs if we want although that isn't as relevant).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(loss)\n\nprintln(weights1.grad)\n# output: [0.15435974752037773 -0.15345737221995426 0.2758968460269525 0.10323749643003427; 0.10696292189737254 -0.18148549954816842 0.20715095141049542 0.12882715523280347; 0.16664054851985355 -0.23974576071873882 0.31358944957671503 0.16792563848560238] \n\nprintln(weights2.grad)\n# output: [1.4368011084584609 -1.2194506134059484 0.01035073085763216 -0.28468347036857006 0.05698224445842545; 1.1107416179804015 -0.8773320376457919 0.008372825855784966 -0.28744229473297594 0.04565988854258152; 1.0101174661066419 -0.8246890949782356 0.007462028540626175 -0.233753522609643 0.04086312294061065; 0.9055652666627538 -0.7839803418601996 0.00643629203797843 -0.16355609507053923 0.035534878230006596]\n\n\nprintln(biases1.grad)\n# output: [0.1994372624495202, -0.31780293172407714, 0.38186796081101293, 0.22451103170524483]\n\nprintln(biases2.grad)\n# output: [0.5783840763706425, -0.4259635644934768, 0.0045351214402561, -0.18149144684303034, 0.024535813525608553]","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Pretty cool! To see how all of this actually works, check out the Under the Hood section. For more extensive tutorials, check out the linear regression and MNIST sections.","category":"page"},{"location":"under_the_hood/#*Value*-composite-type","page":"Under the Hood","title":"Value composite type","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The basic Value composite type looks like this:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"mutable struct Value{opType}\n    data::Float64\n    grad::Float64\n    op::opType\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"There are three fields: data, grad, and op. We've seen two of these fields before, in the Usage section – Value.data and Value.grad, representing the number being stored in the Value and its gradient. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Value.op is something new that we'll be using behind the scenes as part of the gradient tracking. Basically, we'll use it to keep track of what operations and operands were used to create a Value object. To do this, we'll also need to define a new composite type of keep track of these operations. Here's what that looks like:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"struct Operation{FuncType,ArgTypes}\n    op::FuncType\n    args::ArgTypes\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Operation.op will tell us the operation type (addition, multiplication, etc) and Operation.args will point to the operands used in the operation, so that we can access them if we want to.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Next, we need a constructor so that we can initialize Values:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# constructor -- Value(data, grad, op)\nValue(x::Number) = Value(Float64(x), 0.0, nothing)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Looks a bit complicated, I know, but let's break this down. We can initialize a Value object with Value(x) where x is some number. The Value(Float64(x), 0.0, nothing) part means that when we initialze a Value with Value(x), this will set Value.data = Float64(x) (casting x to a Float64 if it's not already), Value.grad = 0.0 and Value.op = nothing. The reason that the operation is set to \"nothing\" here is because we have initialized this Value ourselves rather than creating it as the result of an operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Next, a bit of code so that we can print out values and take a look at them.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base: show\nfunction show(io::IO, value::Value)\n    print(io, \"Value(\",value.data, \")\")\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This lets us print a Value and see the number that it's storing. The import Base: show at the top means that we're using a base Julia function called \"show\" and definining what it will do when we pass a Value as an input. We'll be doing this a lot, for many different base functions.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, so that's our basic setup for Values. At this point, we should be able to run the following code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(4.0)\n\nprintln(x)\n# output: Value(4.0)\n\nprintln(x.data)\n# output: 4.0\n\nprintln(x.grad)\n# output: 0.0\n\nprintln(x.op)\n# output:","category":"page"},{"location":"under_the_hood/#Defining-*Value*-addition","page":"Under the Hood","title":"Defining Value addition","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, so we have our basic building block, but now we want to be able to actually do some calculations with it.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Let's start with addition. Bear with me for a second, I'm gonna give you the full block of code and then we'll go through it bit by bit:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.+\nfunction +(a::Value, b::Value)\n    out = a.data + b.data\n\n    # Value(data, grad, op)\n    result = Value(out, 0.0, Operation(+, (a, b) ))\n\n    return result\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The import Base.+ means that we're importing the base addition function, and +(a::Value, b::Value) means that we're defining what the + operator will do when used on two Values, which we call a and b for the purpose of the function definition. Basically this will allow us to do x + y where x and y are Values rather than regular numbers.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"out = a.data + b.data is how we calculate the actual sum of the two input Values that will be stored in the output Value. Then we create a new Value with result = Value(out, 0.0, Operation(+, (a, b) )). Hopefully this part looks familiar, since we're using the same constructor syntax as before. This will set result.data = out and result.grad = 0.0. The only new part here is that instead of setting result.op = nothing, we're setting result.op = Operation(+, (a, b) ) to specify that this Value was created from an addition operation, and pointing to a and b as the operands, so that we can access them if we want to. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, I know things are getting a little complicated, but setting things up like this will give us a lot of power to go backwards through operations. For example, using only the parts we've written so far you should be able to run this code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# define two Values\nx = Value(2.0)\ny = Value(3.0)\n\n# add them together to get a new Value\nz = x + y\n\nprintln(z)\n# output: Value(5.0)\n\n# inspect the new Value to see what operation produced it\nprintln(z.op.op)\n# output: + (generic function with 194 methods)\n\n# access the Values that were used as operands\nprintln(z.op.args)\n# output: (Value(2.0), Value(3.0))","category":"page"},{"location":"under_the_hood/#Defining-*Value*-backpropagation","page":"Under the Hood","title":"Defining Value backpropagation","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, now let's try to implement backpropagation for the addition operation. Basically the goal here is to be able to calculate the derivative of the output with respect to each of the inputs in the operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Before we actually write the code for this, I'll first show you what we want the end result to look like:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# define two Values\nx = Value(2.0)\ny = Value(3.0)\n\n# add them together to get a new Value\nz = x + y\n\n# calculate the derivative of z with respect to the inputs\nbackward(z)\n\n# the gradient of x tells us the derivative of z with respect to x\nprintln(x.grad)\n# output: 1.0\n\n# dz/dx = 1, meaning an increase of 1 in x will lead to an increase of 1 in z.\n\n# we can also check y.grad if we want to\nprintln(y.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright so that's how the end result should look, but now we need to actually write the code to get there. To do this, we're going to define a function called backprop!() that takes in a Value as an input, and then computes the gradients of the operands that were used to create the Value. This will be an internal function (not actually called by the user), but pretty soon we'll also define another function called backward() which will perform the full backward pass, calling backprop!() along the way. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"One of the cool things about Julia is something called \"multiple dispatch\" – this means that you can define functions with the same name that do things differently based on the type of input that's passed in. If you recall, when we originally defined our Value object, we made it so that the object type contains information about the operation that was used to create it: Value{opType}.  ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"For example:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\nprintln(typeof(x))\n# output: Value{Nothing}","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"We'll begin with the backprop!() function for this simple case, where the Value was not created by an operation, but rather defined by the user. In this case, we will just have the backprop!() function do nothing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"backprop!(val::Value{Nothing}) = nothing","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Now we'll do the harder case, where backprop!() is applied to the result of an addition operation, to calculate the gradients of the operands. Let's look at the full code first, and then we'll discuss what each part is doing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n\n    # update gradient of first operand\n    val.op.args[1].grad += val.grad\n\n    # update gradient of second operand\n    val.op.args[2].grad += val.grad\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"I know, looks pretty confusing! Let's start with the function definition line: backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}. This is just saying that we're definining what the `backprop!() function will do when the input is a Value called val that was created in an addition operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Then, the function updates two things: val.op.args[1].grad and val.op.args[2].grad. This is how we access the gradients of the operands that were used to create val, so that we can update their gradients.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So how do we update the gradients? Well as we mentioned before, for a simple addition operation z = x + y the derivatives of z with respect to both variables are fracdzdx = 1 and fracdzdy = 1. This is because increasing either variable by some amount will cause z to increase by the same amount.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"But wait a minute... the code in our backprop!() function looks way more complicated than that. We're not saying val.op.args[1].grad = 1 and val.op.args[2].grad = 1 (setting both gradients equal to 1). Instead we're saying val.op.args[1].grad += val.grad and val.op.args[2].grad += val.grad – incrementing the operand gradients by the current value of the input Value gradient. The reason we're doing this is because we need to think ahead a little bit. All this complication isn't necessary for our simple z = x + y example, but we're trying to write this function in a general way so that it'll also work for more complicated examples in the futures. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's a more complicated example we want to be able to handle (although still using only addition):","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\ny = Value(3.0)\n\nz = x + y\n\nw = z + x # using x a second time\n\nbackward(w)\n\nprintln(x.grad)\n# output: 2.0\n\nprintln(y.grad)\n# output: 1.0\n\nprintln(z.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This introduces two complications: it has two layers to the calulation, and x is used twice. We use z as an intermediate variable to store the result of x+y, but ultimately we're interested in w = z + x, and we want to find the derivatives fracdwdxand fracdwdy.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This example helps explain the rational for writing backprop!() the way we did. We're calculating the gradients of the operands using the gradient of the input Value so that we can take advantage of the chain rule: we can calculate fracdwdy = fracdwdzfracdzdy. This will involve two calls to backprop!(). First, we'll call backprop!(w) which will calculate the gradient of z, and then we'll call backprop!(z), which will calculate the gradient of y.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Getting the gradient of x is a little more complicated. First, let's quicly prove to ourselves that fracdwdx = 2. The full equation for w is w = z + x. We know that z = x + y, so we can rewrite the equation for w as w = (x + y) + x. Then, taking the derivative with respect to x gives us fracdwdx = 2. Since x contributes to the value of w twice, increasing x by some amount will increase w by twice that amount.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This is the rationale for why our backprop!() function increments the gradients its updating, rather than just setting them to some number. This lets us account for situations where the same Value contributes more than once to the final sum. In our example, x.grad will be updated twice by the backprop!() function – once during the backprop!(w) call and once during the backprop!(z) call. Both of these updates will increase x.grad by 1, leaving us with our final answer of fracdwdx = 2.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Still with me? Alright, last part. We said before that backprop!() is an internal function that won't actually be called by the user. Rather, the user will call a wrapper function backward() on the final sum Value, and that function will do the full backward pass by calling backprop!() as many times as required to calculate the derivatives for all of the input Values. So now we need to write the backward() function.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"First let's take a look at the full code, and then we'll discuss what each part is doing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backward(a::Value)\n\n    function build_topo(v::Value, visited=Value[], topo=Value[])\n        if !(v in visited)\n            push!(visited, v)\n\n            if v.op != nothing\n                for operand in v.op.args\n                    if operand isa Value\n                        build_topo(operand, visited, topo)\n                    end\n                end\n            end\n\n            push!(topo, v)\n        end\n        return topo\n    end\n    \n    topo = build_topo(a)\n\n    a.grad = 1.0\n    for node in reverse(topo)\n        backprop!(node)\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"When we call backward(a) (where a is the final result of some operations between Values) we want two things to happen. First, we want to fill up an array with a itself, and all of the other Value objects that were used to create a, sorted in topological order so that a Value comes after all of the dependencies used to calculate that Value – meaning that a should be the last element in the array since everything else is a dependency of a. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"We can build this array with a recursive depth-first search. This is what the nested function build_topo() is doing. build_topo() returns the topologically sorted array of all the Values, with a at the end. Then we set a.grad = 1, since the derivative of a variable with respect to itself is 1. Finally, we iterate backwards through the list of Values and call backprop!() on each one to update the gradients of its operands. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"That's it! We're done! With the code we've written up to this point, we can do as many addition operations between Values as we want, then do a backward pass on the final sum to calculate its derivative with respect to all the inputs that went into it. We did it!","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Now some of you are probably thinking Wait a minute, we're not done! All we have is an addition operation for Values, and we haven't even started with Tensors yet! Ok, yeah, that's true. I guess what I mean is we're done with the difficult part – the Value object structure, the logic of operation-tracking, and gradient-updating through backpropagation. Now that we're done with all that, adding more Value operations is easy. All we need to know is what the operation does, and how to calculate the derivative for it. Then we can use almost the exact same code we've already written, with only those parts changed. When we finally get to Tensors, the code will be almost exactly the same, except the operations and derivative calculations will be for matrix/vector form.","category":"page"},{"location":"under_the_hood/#Adding-Some-Robustness","page":"Under the Hood","title":"Adding Some Robustness","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, time for a short digression. With our code so far, we can do addition operations between Values. This is a good start, but with a couple more lines of code we can add some robustness, so that we'll be able to do operations between Values and regular numbers.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"First let's take the Value + Number case, like Value(2.0) + 3.0 for example. Here's the code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# addition for Value + Number\nfunction +(a::Value, b::Number)\n    b_value = Value(Float64(b)) # cast b to Value\n    return a + b_value  # use the existing method for Value + Value\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"In this case, we just cast the Number to a Value, and then return the result of the Value + Value operation – easy! Next, let's take the case where we have Number + Value, like 3.0 + Value(2.0) for example. Here's the code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# addition for number + Value\nfunction +(a::Number, b::Value)\n    return b + a # use Value + Number, which then casts the number to Value and does Value + Value\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"In this case, we just switch them around so that it becomes a Value + Number operation, which we've already covered. Now we should be able to run the following code without a problem:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"test = Value(2.0) + Value(3.0) # Value + Value\nprintln(test)\n# output: Value(5.0)\n\ntest = Value(2.0) + 3.0 # Value + Number\nprintln(test)\n# output: Value(5.0)\n\ntest = 2.0 + Value(3.0) # Number + Value\nprintln(test)\n# output: Value(5.0)","category":"page"},{"location":"under_the_hood/#More-*Value*-Operations","page":"Under the Hood","title":"More Value Operations","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, so let's add some more Value operations. We'll start with multiplication. Here's the code, for both the operation and the backward pass:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.*\nfunction *(a::Value, b::Value)\n    out = a.data * b.data\n\n    # Value(data, grad, op)\n    result = Value(out, 0.0, Operation(*, (a, b) ))\n\n    return result\nend\n\n# backprop for multiplication operation\nfunction backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n\n    val.op.args[1].grad += val.op.args[2].data * val.grad\n\n    val.op.args[2].grad += val.op.args[1].data * val.grad\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"That's it! Told you it was easy! The *(a::Value, b::Value) function is almost exactly the same as the addition function we wrote before, except that we're setting out = a.data * b.data and recording the operation as Operation(*, (a, b) ). ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The backprop!() function is also very similar to the one we wrote for addition, with just a couple small changes. First of all, we're now using where {FunType<:typeof(*), ArgTypes} in the funciton definition to specify that this is the version of backprop!() to use when the input variable was created with a multiplication operation (again, the cool thing about multiple dispatch is that we can define several versions of a function with different input types).","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The second minor difference is that we need to change the way the derivates are calculated, since we're dealing with multiplication rather than addition. For a multiplicaiton operation z = xy the derivatives of z with respect to x and y are fracdzdx = yand fracdzdy = x. The two lines inside backprop!() are just saying this in code – the gradient of each operand is incremented by the value of the other operand multiplied by the val.grad (the result of the operation) to allow for the chain rule. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"With the code we've written so far, we can do things like this:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\nm = Value(4.0)\nb = Value(7.0)\n\ny = m*x + b\n\nbackward(y)\n\nprintln(m.grad)\n# output: 2.0\n\nprintln(x.grad)\n# output: 4.0\n\nprintln(b.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"By the way, Julia will still take care of the order of operations for us here, so we could have written y = b + m * x and gotten the same answer.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Just like in the addition case, we can also add some code make our multiplication robust to Value * Number and Number * Value cases:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# Value * number\nfunction *(a::Value, b::Number)\n    b_value = Value(Float64(b))  # cast b to Value\n    return a * b_value # use the existing method for Value * Value\nend\n\n# number * Value\nfunction *(a::Number, b::Value)\n    return b * a # use Value * Number, which then casts the number to Value and does Value * Value\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"A lot of the operations will be like the multiplication case, where we'll need to write a new backprop!() operation. However, sometimes we can find a clever way to do things that avoids this. For example, this is how we'll implement Value subtraction:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.-\n\n# negation\nfunction -(a::Value)\n    return a * -1\nend\n\n# subtraction: Value - Value\nfunction -(a::Value, b::Value)\n    return a + (-b)\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The first function -(a::Value) allows us to negate Values with a minus sign. This can be done by multiplying the Value by -1, an operation we can already do with our *(a::Value, b::Number) function. The second function -(a::Value, b::Value) allows us to do subtraction with Values by negating the second Value and then adding them together. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Pretty clever, right? This way we don't need to write a new backprop!() function for subtraction, because we've turned the subtraction operation into a combination of multiplication and addition. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Anyway, from here it's just a matter of adding more operations so that we can do more calculations with our Values. There are the operations currently supported:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Addition\nSubtraction\nMultiplication\nDivision\nExponents\ne^x\nlog()\ntanh()","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"If you've understood everything up to this point, you should be able to read all the source code for the Value objects and make sense of it. If there are any operations you'd like to see added, either let me know and I'll try to add them, or you can also write them yourself and submit a pull request!","category":"page"},{"location":"under_the_hood/#*Tensor*-Objects","page":"Under the Hood","title":"Tensor Objects","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Tensors work almost exactly the same way as Values, except with a little bit of extra complications that come with dealing with vectors and matrices. But the fundamentals are basically the same. We'll track operations with our Operation objects, override several base Julia functions to work for Tensor operations, and implement the backward pass with an internal backprop!() function and a user-facing backward() function.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The Operation object structure is the same as before:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"struct Operation{FuncType,ArgTypes}\n    op::FuncType\n    args::ArgTypes\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"And here's our definition of the Tensor object structure:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"mutable struct Tensor{opType}\n    data::Union{Array{Float64,1},Array{Float64,2}}\n    grad::Union{Array{Float64,1},Array{Float64,2}}\n    op::opType\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"As you can see, it's very similary to the Value object, except that the Tensor.data and Tensor.grad fields are arrays (either one-dimensional or two-dimensional) rather than numbers. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the Tensor constructor:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Tensor(x::Union{Array{Float64,1},Array{Float64,2}}) = Tensor(x, zeros(Float64, size(x)), nothing)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, same basic idea as the Value constructor, except that we're dealing with arrays instead of numbers.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Just a couple more quick things to take care of before we start defining operations. The following code lets us print out Tensors and also sets the backprop!() function to be nothing in cases where a Tensor was defined by the user rather than being created in an operation (again, same as with Values):","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.show\nfunction show(io::IO, tensor::Tensor)\n    print(io, \"Tensor(\",tensor.data, \")\")\nend\n\nbackprop!(tensor::Tensor{Nothing}) = nothing","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, now let's try defining a Tensor operation. When we were learning about how Values work we started with addition because that seemed like the easiest. But for Tensors, addition will actually be a little tough because of some shape-broadcasting we'll need to do. So we'll start with matrix multiplication, since that will be easier. Here's the code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.*\nfunction *(a::Tensor, b::Tensor)\n\n    out = a.data * b.data\n\n    # Tensor(data, grad, op)\n    result = Tensor(out, zeros(Float64, size(out)), Operation(*, (a, b)))\n\n    return result\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Very similar to what we were doing with Values, except that this time it's matrix multiplication. But same idea. We do the matrix multiplication with out = a.data * b.data. Then we store the resulting matrix out along with an emptry gradient size(out)) in a new Tensor called result, and record the operation as Operation(*, (a, b)). ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"And here's the backprop!() function:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n\n    tensor.op.args[1].grad += tensor.grad * transpose(tensor.op.args[2].data)\n\n    tensor.op.args[2].grad += transpose(tensor.op.args[1].data) * tensor.grad \n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, basically the same idea as with the Value backprop!() functions. The only difficult part is that now everything has to be done for matrices, which makes the actual calculations more complicated and less intuitive. If you want, you can always work out a simple example on paper to prove to yourself that the gradient updates in this backprop!() function are actually correct. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Finally, here's the code for the full backward pass:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backward(a::Tensor)\n\n    function build_topo(v::Tensor, visited=Tensor[], topo=Tensor[])\n        if !(v in visited)\n            push!(visited, v)\n\n            if v.op != nothing\n                for operand in v.op.args\n                    if operand isa Tensor\n                        build_topo(operand, visited, topo)\n                    end\n                end\n            end\n\n            push!(topo, v)\n        end\n        return topo\n    end\n    \n    topo = build_topo(a)\n\n    a.grad .= 1.0\n    for node in reverse(topo)\n        backprop!(node)\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, almost exactly the same as for the Values.","category":"page"},{"location":"under_the_hood/#More-*Tensor*-operations","page":"Under the Hood","title":"More Tensor operations","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"For the sake of completeness, I'm going to give you the code for all of the Tensor operations needed to write a basic neural network. For now, working through the details to understand them will be left as an exercise for the reader, although I'll probably try to come back to this section to write a more complete description when I have some more free time.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's addition:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.+\nfunction +(a::Tensor, b::Tensor)\n\n    if length(size(a.data)) == length(size(b.data))\n        out = a.data .+ b.data\n    elseif length(size(a.data)) > length(size(b.data))\n        # a is 2D, b is 1D\n        out = a.data .+ transpose(b.data)\n    else\n        # a is 1D, b is 2D\n        out = b.data .+ transpose(a.data)\n    end\n\n    # Tensor(data, grad, op)\n    result = Tensor(out, zeros(Float64, size(out)), Operation(+, (a, b)))\n\n    return result\nend\n\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n\n\n    if length(size(tensor.grad)) > length(size(tensor.op.args[1].data))\n        tensor.op.args[1].grad += dropdims(sum(tensor.grad, dims=1), dims=1) # need dropdims to make this size (x,) rather than size (1,x)\n    else\n        tensor.op.args[1].grad += ones(size(tensor.op.args[1].data)) .* tensor.grad\n    end\n\n    if length(size(tensor.grad)) > length(size(tensor.op.args[2].data))\n        tensor.op.args[2].grad += dropdims(sum(tensor.grad, dims=1), dims=1) # need dropdims to make this size (x,) rather than size (1,x)\n    else\n        tensor.op.args[2].grad += ones(size(tensor.op.args[2].data)) .* tensor.grad\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Hint about this one: the complicated parts are related to broadcasting so that we can add add a one-dimensional vector to a two-dimensional matrix (like adding biases in a neural net).","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the ReLU activation function:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function relu(a::Tensor)\n\n    out = max.(a.data,0)\n\n    # Tensor(data, grad, op)\n    result = Tensor(out, zeros(Float64, size(out)), Operation(relu, (a,)))\n\n    return result\nend\n\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(relu), ArgTypes}\n\n    tensor.op.args[1].grad += (tensor.op.args[1].data .> 0) .* tensor.grad\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the combined softmax activation and cross entropy loss:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function softmax_crossentropy(a::Tensor,y_true::Union{Array{Int,2},Array{Float64,2}}; grad::Bool=true)\n\n    ## implementing softmax activation and cross entropy loss separately leads to very complicated gradients\n    ## but combining them makes the gradient a lot easier to deal with\n\n    ## credit to Sendex and his textbook for teaching me this part\n    ## great textbook for doing this stuff in Python, you can get it here:\n    ## https://nnfs.io/\n\n    # softmax activation\n    exp_values = exp.(a.data .- maximum(a.data, dims=2))\n    probs = exp_values ./ sum(exp_values, dims=2)\n    \n    ## crossentropy - sample losses\n    samples = size(probs, 1)\n    probs_clipped = clamp.(probs, 1e-7, 1 - 1e-7)\n    # deal with 0s\n\n\n    # basically just returns an array with the probability of the correct answer for each batch\n    correct_confidences = sum(probs_clipped .* y_true, dims=2)\n\n    # negative log likelihood\n    sample_losses = -log.(correct_confidences)\n\n\n    # loss_mean\n    out = [mean(sample_losses)]\n\n\n\n    if grad\n\n        # it's easier to do the grad calculation here because doing it seperately will involve redoing a lot of calculations\n\n        samples = size(probs, 1)\n\n        # convert from one-hot to index list\n        y_true_argmax = argmax(y_true, dims=2)\n\n        a.grad = copy(probs)\n        for samp_ind in 1:samples\n            a.grad[samp_ind, y_true_argmax[samp_ind][2]] -= 1\n            ## this syntax y_true_argmax[i][2] is just to get the column index of the true value\n        end\n        a.grad ./= samples\n\n\n    end\n\n    # Tensor(data, grad, op)\n    result = Tensor(out, zeros(Float64, size(out)), Operation(softmax_crossentropy, (a,)))\n\n    return result\nend\n\n\n\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(softmax_crossentropy), ArgTypes}\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Note: this is the most complicated one by far, and is also the odd-one-out in that the gradient is actually calculated during the forward pass (unless told not to), which the backprop!() function just does nothing. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So yeah, sorry to leave you guys with a complicated one here. I'll probably come back later and try to write a more thorough description of this one when I have some more free time in the future.","category":"page"},{"location":"#SimpleGrad.jl-Basics","page":"Welcome","title":"SimpleGrad.jl Basics","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Hi, thanks for checking out SimpleGrad.jl! This is a machine learning package that can be used for a variety of applications that involve gradient-tracking and backpropagation, including neural networks. ","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"However, unlike most other machine learning packages, the primary goal of SimpleGrad is to be educational. So if you're looking for the best possible speed and performance, you probably shouldn't use SimpleGrad (maybe check out Flux.jl instead). But if you're new to machine learning or Julia (or both) and want to understand how things work, then I think you'll find SimpleGrad useful! ","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"In fact, I started this project because I'm a beginner at machine learning and Julia myself, and figured the best way to learn was by doing (or at least, attempting). This package is the result of my own attempt to learn about machine learning, gradient-tracking, and multiple dispatch (Julia's alternative to object oriented programming), and my hope is that it will be useful for other beginners who are trying to learn. The idea is that the source code should be useful for some basic applications, while also being simple enough to read, understand, and edit/customize if you want to.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"SimpleGrad includes two main composite types (basically Julia's version of objects/classes): Values and Tensors. Values store single numbers, and Tensors store arrays of numbers. Both Values and Tensors support a variety of operations, which are automatically tracked so that the gradients can be calculated with a backward pass.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"In the Usage section, we'll cover how to actually use Values and Tensors to do calculations and calculate gradients. Then in the Under the Hood section, we'll take a look at the source code and talk about how it works. Lastly, I've also included two tutorials (and will probably add more later): linear regression and MNIST.","category":"page"},{"location":"#Installation","page":"Welcome","title":"Installation","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"If everything's working properly, you should be able to install SimpleGrad right from the Julia package manager like this:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"using Pkg\nPkg.add(\"SimpleGrad\")","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"However, if that gives you any problems then it probably means that I screwed something up. Sorry! I'm a beginner here and this is my first time trying to make a Julia package. So as a backup, you can also just grab the source code file and put it on your computer. Then, just direct Julia to it's location in order to use it. Like this, but replace the location with its location on your computer:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"push!(LOAD_PATH, \"/Users/mikesaint-antoine/Desktop/\") \n# change this to the location of the folder where SimpleGrad.jl is on your computer\n\nusing SimpleGrad","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"In fact, even if installation through Pkg is working, grabbing the source code directly might be the better approach for educational purposes, so that you can easily edit it and play around with it.","category":"page"},{"location":"#Credits","page":"Welcome","title":"Credits","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"In this section I'd like to credit/cite a couple people who taught me all this stuff.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Andrej Karpathy's Micrograd video lesson\nHarrison Kinsley (Sentdex on Youtube) neural net textbook\nu/Bob_Dieter's reddit comment – my first attempt at writing a Julia gradient tracker was this project called mikerograd. At the time I was a total beginner, coming from Python, and really didn't know how to use Julia's multiple dispatch, so I tried to write it similarly to how it would look in OOP / Python style. I posted it on Reddit, and u/Bob_Dieter replied with a series of very helpful comments about how it could be done better with multiple dispatch / function overloading. Thanks Bob!","category":"page"},{"location":"tutorials/linear_regression/#Linear-Regression-Example","page":"Linear Regression","title":"Linear Regression Example","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's try using the Value class to fit a line to data, using gradient descent.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"First, we'll make up some fake data of two things that are linearly related: height and basketball-skills.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"using Random\nRandom.seed!(1234)\n\nheights = Float64[]\nfor count in 1:79\n    push!(heights, rand(40:90))\nend\n\n\n# TRUE PARAMS y = m*x + b\nm = 2\nb = 10\n\n\n\nskills = Float64[]\n\nfor height in heights\n    skill = m * height + b + randn() * 7.0\n    push!(skills, skill)\nend","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Just for fun, I'll add myself to this dataset. I'm 72 inches tall, and extremely bad at basketball lol","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"push!(heights, 72)\npush!(skills, 75)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Now let's plot the data just to take a look at it:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"using Plots\nscatter(heights, skills, legend=false, markersize=3, color=:black, xlabel=\"Height (inches)\", ylabel=\"Basketball Skills\",dpi=300)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: scatter_plot)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Ok, now let's see if we can use the Value class to fit a line to this data.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"heights = [Value(item) for item in heights]\nskills = [Value(item) for item in skills]","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"lr = 0.000002\nruns = 100000\n\n# initial guesses to start with\nm_guess = Value(0)\nb_guess = Value(0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"for run in 1:runs\n\n    m_guess.grad = 0\n    b_guess.grad = 0\n\n    global loss = Value(0)\n\n    for i in 1:length(heights)\n\n        skill_pred = heights[i] * m_guess + b_guess\n        loss_to_add = (skill_pred - skills[i])^2\n        global loss += loss_to_add\n    end\n\n    \n    backward(loss)\n\n    m_guess.data -= m_guess.grad * lr\n    b_guess.data -= b_guess.grad * lr\n\nend","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's see where our guesses for m and b are at now.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"println(m_guess)\n# output: Value(1.9906384976156302)\n\nprintln(b_guess)\n# output: Value(10.133894222774007)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Pretty close to the real values that we originally used to make the data!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can also plot the fit line with these m and b parameters.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"heights_data = [item.data for item in heights] # remember heights is full of Values, so need to do this to get the numbers\n\nx_line = minimum(heights_data):maximum(heights_data)\ny_line = m_guess.data * x_line .+ b_guess.data\nplot!(x_line, y_line, linewidth=2, color=:blue)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: scatter_plot)","category":"page"},{"location":"tutorials/mnist/#MNIST-Example","page":"MNIST","title":"MNIST Example","text":"","category":"section"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Lastly, let's try out a real neural net example – solving the MNIST image classification problem.","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"You can download the MNIST data in CSV format here: https://www.kaggle.com/datasets/oddrationale/mnist-in-csv","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"First, we'll read in the training and testing data...","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"## read training data\n\nX = []\ny = []\nglobal first_row = true\nopen(\"mnist_data/mnist_train.csv\", \"r\") do file\n    for line in eachline(file)\n\n        if first_row  # Skip the first row\n            global first_row = false\n            continue\n        end\n\n        # Split the line by comma and strip whitespace\n        row = parse.(Float64, strip.(split(line, ',')))\n\n        push!(y, row[1])\n        push!(X, row[2:length(row)])\n    end\nend\n\nX= hcat(X...)';\nX = X / 255.0;\n\n\n\n## read testing data\n\nX_test = []\ny_test = []\nglobal first_row = true\nopen(\"mnist_data/mnist_test.csv\", \"r\") do file\n    for line in eachline(file)\n\n        if first_row  # Skip the first row\n            global first_row = false\n            continue\n        end\n\n        # Split the line by comma and strip whitespace\n        row = parse.(Float64, strip.(split(line, ',')))\n\n        push!(y_test, row[1])\n        push!(X_test, row[2:length(row)])\n    end\nend\n\nX_test = hcat(X_test...)';\nX_test = X_test / 255.0;","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Next, we define the model...","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"## define model\n\nweights1 = Tensor(0.01 * rand(784, 128));\nweights2 = Tensor(0.01 * rand(128, 10));\n\nbiases1 = Tensor(zeros(128));\nbiases2 = Tensor(zeros(10));\n\n\nbatch_size = 100;\nnum_classes = 10;  # total number of classes\nlr = 0.1;\nepochs = 2;","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Now, we train the model...","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"global run = 1\nfor epoch in 1:epochs\n\n    for i in 1:batch_size:size(X,1)\n\n\n        ## get current batch\n        batch_X = X[i:i+batch_size-1, :]\n        batch_X = Tensor(batch_X)\n        batch_y = y[i:i+batch_size-1]\n\n\n\n        ## convert batch_y to one-hot\n        batch_y_one_hot = zeros(batch_size,num_classes)\n        for batch_ind in 1:batch_size\n            batch_y_one_hot[batch_ind,Int.(batch_y)[batch_ind]+1] = 1\n        end\n\n\n\n        ## zero grads\n        weights1.grad .= 0\n        weights2.grad .= 0\n        biases1.grad .= 0\n        biases2.grad .= 0\n\n\n\n        ## forward pass\n        layer1_out = relu(batch_X * weights1 + biases1)\n        layer2_out = layer1_out * weights2 + biases2\n        loss = softmax_crossentropy(layer2_out,batch_y_one_hot)\n\n\n\n        ## backward pass\n        backward(loss)\n\n\n        ## update params\n        weights1.data -= weights1.grad .* lr\n        weights2.data -= weights2.grad .* lr\n        biases1.data -= biases1.grad .* lr\n        biases2.data -= biases2.grad .* lr\n\n\n        if run % 10 == 0\n            println(\"Epoch: $epoch, run: $run, loss: $(round(loss.data[1], digits=3))\")\n        end\n        \n        global run += 1\n\n    end\nend","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Finally, we check out performance on the testing set...","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"global correct = 0\nglobal total = 0\nfor i in 1:length(y_test)\n    X_in = X_test[i:i,:] ## need to keep this (1,784), not (784,)\n    X_in = Tensor(X_in)\n    y_true = y_test[i]\n\n    layer1_out = relu(X_in * weights1 + biases1)\n    layer2_out = layer1_out * weights2 + biases2\n\n\n    pred_argmax = argmax(layer2_out.data, dims=2)[1][2]\n\n    if pred_argmax-1 == y_true\n        global correct +=1\n    end\n    global total += 1\n\nend\n\nprintln(correct/total)","category":"page"}]
}
