var documenterSearchIndex = {"docs":
[{"location":"usage/#Quick-Intro","page":"Usage","title":"Quick Intro","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"SimpleGrad includes two main composite types (basically Julia's version of objects/classes): Values and Tensors. Values store single numbers and Tensors store arrays of numbers. Both Values and Tensors support a variety of operations, which are automatically tracked so that the gradients can be calculated with a backward pass.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"In this section, we'll cover how to actually use Values and Tensors to do calculations and compute gradients. Then in the Under the Hood section, we'll take a look at the source code and talk about how it works. I've also included two tutorials for extra practice (and will probably add more later): linear regression and MNIST.","category":"page"},{"location":"usage/#*Values*","page":"Usage","title":"Values","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's start with the Value composite type. Here's how you define a Value","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using SimpleGrad\n\nx = Value(4.0)\n\nprintln(x)\n# output: Value(4.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Values can store numbers, perform operations, and automatically track the gradients of the variables involved.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here's how you take a look at the number a Value is storing (called Value.data), and its gradient (called Value.grad):","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.data) # the number\n# output: 4.0\n\nprintln(x.grad) # the gradient\n# output: 0.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here, x.data == 4.0 because the Value x is storing the number 4.0, and x.grad == 0.0 is a placeholder for the gradient, which could eventually change if we do some operations and eventually back-calculate the gradient.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next let's try an operation. We'll define another Value called y, add it to x, and save the result as z.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"y = Value(3.0)\nz = x + y\n\nprintln(z)\n# output: Value(7.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Pretty simple so far, right? But here's the cool part â€“ we can now do a backward pass to calculate the derivative of z with respect to x and y. Here's how we do that:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now, the grad fields of x and y are populated, and will tell us the derivative of z with respect to each of the inputs x and y.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.grad) # dz/dx = 1, meaning an increase of 1 in x will lead to an increase of 1 in z.\n# output: 1.0\n\nprintln(y.grad) # dz/dy = 1, meaning an increase of 1 in y will lead to an increase of 1 in z.\n# output: 1.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"In mathematical terms, we're considering the equation z = x + y and are interested in the derivatives fracdzdx and fracdzdy. x.grad == 1 tells us that fracdzdx = 1 and y.grad == 1 tells us that fracdzdy = 1 for the values of x and y that we've defined in our code (and in this specific example, for all values of x and y). If you're rusty on the calculus, you can also think of it this way: increasing x by 1 will cause z to increase by 1, and increasing y by 1 will also cause z to increase by 1.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"So that's the basic functionality of the Value class. We can store store numbers, do operations, and track the derivative of the output with respect to all of the inputs. This allows us to, for example, minimize a loss function through gradient-descent, by tracking the derivative of the loss with respect to the model parameters, and then updating those parameters so that the loss decreases.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here's a list of the operations currently supported:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Addition\nSubtraction\nMultiplication\nDivision\nExponents\ne^x\nlog()\ntanh()","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's test a couple of them out. We've already done addition, so let's try subtraction.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(10.0)\ny = Value(3.0)\nz = x - y\n\nprintln(z)\n# output: Value(7.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"If you want, you can try backward(z), and you should be able to find x.grad == 1 meaning that  fracdzdx = 1, and y.grad == -1 meaning that fracdzdy = -1. But I'll skip over that for now.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Next let's try multiplication.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(6.0)\ny = Value(2.0)\nz = x * y\n\nprintln(z)\n# output: Value(12.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And again, we can get the derivative with of z with respect to x and y.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = y = 2\n# output: 2.0\n\nprintln(y.grad) # dz/dy = x = 6\n# output: 6.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Alright, so far so good! Let's try division now:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(15.0)\ny = Value(5.0)\nz = x / y\n\nprintln(z)\n# output: Value(3.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 1/5 = 0.2\n# output: 0.2\n\nprintln(y.grad) # dz/dy = -15 / x^2 = -0.6\n# output: -0.6","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now let's try exponents. NOTE: for this function, the exponent must be an Integer, NOT a Value or Float. Might work on fixing this later.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(5.0)\ny = 2 # NOTE - exponent must be an Integer, not a Value or Float\nz = x^y\n\nprintln(z)\n# output: Value(25.0)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 2x = 10\n# output: 10.0","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now for the exponential function e^x, which we'll call exp().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(2.0)\nz = exp(x)\n\nprintln(z)\n# output: Value(7.38905609893065)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = e^x = (same thing we got for above)\n# output: 7.38905609893065","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Ok, now for the natural logarithm, which we call log().","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(10.0)\nz = log(x)\n\nprintln(z)\n# output: Value(2.302585092994046)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\n\nprintln(x.grad) # dz/dx = 1/x = 0.1\n# output: 0.1","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Lastly, the tanh() function. Personally my trig is pretty rusty and I don't use this function very often, but I'm including it because it was in Andrej Karpathy's Micrograd, which the SimpleGrad Value is based on. tanh()  is useful as a possible activation function for a linear layer of neurons, to add nonlinearity and bound the layer outputs on [-1, 1].","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Value(3.0)\nz = tanh(x)\n\nprintln(z)\n# output: Value(0.9950547536867305)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"And here's the backward pass:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(z)\nprintln(x.grad) # dz/dx = 1 - tanh^2(x) = ????\n# output: 0.009866037165440211","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"So far these examples have been pretty simple. But as long as we're using these simple functions, we can combine them in pretty complicated ways. The gradients can still be calculated for all the inputs, using backpropagation and the chain rule of derivatives.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Let's try out a complicated example to see this...","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"input1 = Value(2.3)\ninput2 = Value(-3.5)\ninput3 = Value(3.9)\n\nweight1 = Value(-0.8)\nweight2 = Value(1.8)\nweight3 = Value(3.0)\n\nbias = Value(-3.2)\n\ny_pred = tanh(input1*weight1 + input2*weight2 + input3*weight3 + bias)\ny_true = Value(0.8)\n\nloss = (y_pred - y_true)^2\n\nprintln(loss)\n# output: Value(0.20683027474728832)","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Here we're using 3 inputs, 3 weights, a bias, and a tanh() activation function to come up with some prediction in a regression problem, and calculating a loss by comparing it to the target value.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Even though this looks pretty complicated, we can still use backward(loss) to calculate the derivative of the loss with respect to everything.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(loss)\n\nprintln(weight1.grad) # dloss/dweight1\n# output: -1.8427042527651991\n\nprintln(weight2.grad) # dloss/dweight2\n# output: 2.80411516725139\n\nprintln(weight3.grad) # dloss/dweight3\n# output: -3.12458547208012\n\nprintln(bias.grad) # dloss/dbias\n# output: -0.8011757620718257","category":"page"},{"location":"usage/#*Tensors*","page":"Usage","title":"Tensors","text":"","category":"section"},{"location":"usage/","page":"Usage","title":"Usage","text":"Values are pretty useful for some specific cases, but unfortunately their scalar-valued calculations will be too slow when it comes to implementing even a pretty basic neural network. So in addition to Values, we also have our Tensor composite type, which stores data in array format (either one-dimensional or two-dimensional).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"We can define a Tensor like this:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"x = Tensor([2.0, 3.0, 4.0])\n\nprintln(x)\n# output: Tensor([2.0, 3.0, 4.0])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Similarly to Values, Tensors also have fields called data and grad that store their arrays of numbers and gradients.","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"println(x.data)\n# output: [2.0, 3.0, 4.0]\n\nprintln(x.grad)\n# output: [0.0, 0.0, 0.0]","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Right now the Tensor class pretty much has the bare minimum needed to implement a simple neural network, although I'm probably going to add more in the future. Here's a list of the operations currently supported:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Addition\nMatrix Multiplication\nReLU\nSoftmax Activation / Cross Entropy Loss Combination","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Rather than testing out all of these individually, let's see if we can save some time by testing them all out at once:","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"using Random\nRandom.seed!(1234)\n\ninputs = Tensor(rand(2, 3)) # Matrix with shape (2,3) -- 2 batches, 3 input features per batch\nweights1 = Tensor(rand(3, 4)) # Matrix with shape (3,4) -- takes 3 inputs, has 4 neurons\nweights2 = Tensor(rand( 4, 5)) # Matrix with shape (4,5) -- takes 4 inputs, has 5 neurons\nbiases1 = Tensor([1.0, 1.0, 1.0, 1.0]) # Bias vector for first layer neurons\nbiases2 = Tensor([1.0, 1.0, 1.0, 1.0, 1.0]) # Bias vector for second layer neurons\n\n\nlayer1_out = relu(inputs * weights1 + biases1)\n\nlayer2_out = layer1_out * weights2 + biases2\n\n\n# important -- correct classes should be one-hot encoded and NOT a Tensor, just a regular matrix.\ny_true = [0 1 0 0 0;\n          0 0 0 1 0]\n\nloss = softmax_crossentropy(layer2_out,y_true)\n\n\n\nprintln(loss)\n# output: Tensor([2.137377648400186])","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Now we can find the derivative of the loss with respect to the weights and biases (and inputs if we want although that isn't as relevant).","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"backward(loss)\n\nprintln(weights1.grad)\n# output: [-0.3908952261176255 0.12683215951155127 0.2416920583878119 0.38808820865148697; -0.29634913482989794 0.07488805376600344 0.14838075027585607 0.29197521584353536; -0.4750896539667244 0.14050215019726503 0.27138275497284103 0.4702367656227933]\n\nprintln(weights2.grad)\n# output: [0.863617972699941 -0.2976748175494542 0.023649534838850777 -0.6817622076321975 0.09216951764285997; 1.0039983446760201 -0.3429788768507116 0.02749410384710956 -0.7956612611906658 0.10714768951824792; 1.19691493443326 -0.43168111913872687 0.03277440852700377 -0.9257730447883243 0.1277648209667876; 1.0153161801798791 -0.3479645917015216 0.02780390869850365 -0.8035124553248412 0.10835695814798015]\n\n\nprintln(biases1.grad)\n# output: [-0.5767635099832011 0.16409112495001651 0.31884196813703536 0.5701877733300129]\n\nprintln(biases2.grad)\n# output: [0.5785120187360111 -0.20248891413334258 0.01584176327066983 -0.453610397799674 0.06174552992633567]","category":"page"},{"location":"usage/","page":"Usage","title":"Usage","text":"Pretty cool! To see how all of this actually works, check out the Under the Hood section. For more extensive tutorials, check out the linear regression and MNIST sections.","category":"page"},{"location":"under_the_hood/#*Value*-composite-type","page":"Under the Hood","title":"Value composite type","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The basic Value composite type looks like this:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"mutable struct Value{opType} <: Number\n    data::Float64\n    grad::Float64\n    op::opType\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The <: Number part means that Value is a subtype of Number. Don't worry about this for now, but we'll discuss it more later.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"There are three fields: data, grad, and op. We've seen two of these fields before, in the Usage section â€“ Value.data and Value.grad, representing the number being stored in the Value and its gradient. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Value.op is something new that we'll be using behind the scenes as part of the gradient tracking. Basically, we'll use it to keep track of what operations and operands were used to create a Value object. To do this, we'll also need to define a new composite type of keep track of these operations. Here's what that looks like:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"struct Operation{FuncType,ArgTypes}\n    op::FuncType\n    args::ArgTypes\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Operation.op will tell us the operation type (addition, multiplication, etc) and Operation.args will point to the operands used in the operation, so that we can access them if we want to.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Next, we need a constructor so that we can initialize Values:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# constructor -- Value(data, grad, op)\nValue(x::Number) = Value(Float64(x), 0.0, nothing)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Looks a bit complicated, I know, but let's break this down. We can initialize a Value object with Value(x) where x is some number. The Value(Float64(x), 0.0, nothing) part means that when we initialze a Value with Value(x), this will set Value.data = Float64(x) (casting x to a Float64 if it's not already), Value.grad = 0.0 and Value.op = nothing. The reason that the operation is set to \"nothing\" here is because we have initialized this Value ourselves rather than creating it as the result of an operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Next, a bit of code so that we can print out values and take a look at them.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base: show\nfunction show(io::IO, value::Value)\n    print(io, \"Value(\",value.data, \")\")\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This lets us print a Value and see the number that it's storing. The import Base: show at the top means that we're using a base Julia function called \"show\" and definining what it will do when we pass a Value as an input. We'll be doing this a lot, for many different base functions. In the source code, we import all of these functions with one line at the top of the file.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"One more quick formality:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.==\nfunction ==(a::Value, b::Value)\n    return a===b\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The triple-equal-sign === checks if two variables are pointing to the same object in memory, so this code means that for two values a and b, the equality check a==b will return true only if they are pointing to the same Value object, and will return false if they're pointing to two different objects (even if both store the same number, gradient, and operation history). ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, so that's our basic setup for Values. At this point, we should be able to run the following code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(4.0)\n\nprintln(x)\n# output: Value(4.0)\n\nprintln(x.data)\n# output: 4.0\n\nprintln(x.grad)\n# output: 0.0\n\nprintln(x.op)\n# output:\n\ny = Value(4.0)\n\nprintln(x==y)\n# output: false\n\nz = x # z and x point to the same Value object\n\nprintln(x==z)\n# output: true","category":"page"},{"location":"under_the_hood/#Defining-*Value*-addition","page":"Under the Hood","title":"Defining Value addition","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, so we have our basic building block, but now we want to be able to actually do some calculations with it.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Let's start with addition. Bear with me for a second, I'm gonna give you the full block of code and then we'll go through it bit by bit:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.+\nfunction +(a::Value, b::Value)\n\n    out = a.data + b.data\n    result = Value(out, 0.0, Operation(+, (a, b) )) # Value(data, grad, op)\n    return result\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The import Base.+ means that we're importing the base addition function, and +(a::Value, b::Value) means that we're defining what the + operator will do when used on two Values, which we call a and b for the purpose of the function definition. Basically this will allow us to do x + y where x and y are Values rather than regular numbers. Again, in the source code all these imports are in one line at the top.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"out = a.data + b.data is how we calculate the actual sum of the two input Values that will be stored in the output Value. Then we create a new Value with result = Value(out, 0.0, Operation(+, (a, b) )). Hopefully this part looks familiar, since we're using the same constructor syntax as before. This will set result.data = out and result.grad = 0.0. The only new part here is that instead of setting result.op = nothing, we're setting result.op = Operation(+, (a, b) ) to specify that this Value was created from an addition operation, and pointing to a and b as the operands, so that we can access them if we want to. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, I know things are getting a little complicated, but setting things up like this will give us a lot of power to go backwards through operations. For example, using only the parts we've written so far you should be able to run this code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# define two Values\nx = Value(2.0)\ny = Value(3.0)\n\n# add them together to get a new Value\nz = x + y\n\nprintln(z)\n# output: Value(5.0)\n\n# inspect the new Value to see what operation produced it\nprintln(z.op.op)\n# output: + (generic function with 194 methods)\n\n# access the Values that were used as operands\nprintln(z.op.args)\n# output: (Value(2.0), Value(3.0))","category":"page"},{"location":"under_the_hood/#Defining-*Value*-backpropagation","page":"Under the Hood","title":"Defining Value backpropagation","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, now let's try to implement backpropagation for the addition operation. Basically the goal here is to be able to calculate the derivative of the output with respect to each of the inputs in the operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Before we actually write the code for this, I'll first show you what we want the end result to look like:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"# define two Values\nx = Value(2.0)\ny = Value(3.0)\n\n# add them together to get a new Value\nz = x + y\n\n# calculate the derivative of z with respect to the inputs\nbackward(z)\n\n# the gradient of x tells us the derivative of z with respect to x\nprintln(x.grad)\n# output: 1.0\n\n# dz/dx = 1, meaning an increase of 1 in x will lead to an increase of 1 in z.\n\n# we can also check y.grad if we want to\nprintln(y.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright so that's how the end result should look, but now we need to actually write the code to get there. To do this, we're going to define a function called backprop!() that takes in a Value as an input, and then computes the gradients of the operands that were used to create the Value. This will be an internal function (not actually called by the user), but pretty soon we'll also define another function called backward() which will perform the full backward pass, calling backprop!() along the way. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"One of the cool things about Julia is something called \"multiple dispatch\" â€“ this means that you can define functions with the same name that do things differently based on the type of input that's passed in. If you recall, when we originally defined our Value object, we made it so that the object type contains information about the operation that was used to create it: Value{opType}.  ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"For example:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\nprintln(typeof(x))\n# output: Value{Nothing}","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"We'll begin with the backprop!() function for this simple case, where the Value was not created by an operation, but rather defined by the user. In this case, we will just have the backprop!() function do nothing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"backprop!(val::Value{Nothing}) = nothing","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Now we'll do the harder case, where backprop!() is applied to the result of an addition operation, to calculate the gradients of the operands. Let's look at the full code first, and then we'll discuss what each part is doing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n\n    val.op.args[1].grad += val.grad # update gradient of first operand\n    val.op.args[2].grad += val.grad # update gradient of second operand\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"I know, looks pretty confusing! Let's start with the function definition line: backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}. This is just saying that we're definining what the `backprop!() function will do when the input is a Value called val that was created in an addition operation.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Then, the function updates two things: val.op.args[1].grad and val.op.args[2].grad. This is how we access the gradients of the operands that were used to create val, so that we can update their gradients.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So how do we update the gradients? Well as we mentioned before, for a simple addition operation z = x + y the derivatives of z with respect to both variables are fracdzdx = 1 and fracdzdy = 1. This is because increasing either variable by some amount will cause z to increase by the same amount.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"But wait a minute... the code in our backprop!() function looks way more complicated than that. We're not saying val.op.args[1].grad = 1 and val.op.args[2].grad = 1 (setting both gradients equal to 1). Instead we're saying val.op.args[1].grad += val.grad and val.op.args[2].grad += val.grad â€“ incrementing the operand gradients by the current value of the input Value gradient. The reason we're doing this is because we need to think ahead a little bit. All this complication isn't necessary for our simple z = x + y example, but we're trying to write this function in a general way so that it'll also work for more complicated examples in the futures. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's a more complicated example we want to be able to handle (although still using only addition):","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\ny = Value(3.0)\n\nz = x + y\n\nw = z + x # using x a second time\n\nbackward(w)\n\nprintln(x.grad)\n# output: 2.0\n\nprintln(y.grad)\n# output: 1.0\n\nprintln(z.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This introduces two complications: it has two layers to the calulation, and x is used twice. We use z as an intermediate variable to store the result of x+y, but ultimately we're interested in w = z + x, and we want to find the derivatives fracdwdxand fracdwdy.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This example helps explain the rational for writing backprop!() the way we did. We're calculating the gradients of the operands using the gradient of the input Value so that we can take advantage of the chain rule: we can calculate fracdwdy = fracdwdzfracdzdy. This will involve two calls to backprop!(). First, we'll call backprop!(w) which will calculate the gradient of z, and then we'll call backprop!(z), which will calculate the gradient of y.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Getting the gradient of x is a little more complicated. First, let's quicly prove to ourselves that fracdwdx = 2. The full equation for w is w = z + x. We know that z = x + y, so we can rewrite the equation for w as w = (x + y) + x. Then, taking the derivative with respect to x gives us fracdwdx = 2. Since x contributes to the value of w twice, increasing x by some amount will increase w by twice that amount.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This is the rationale for why our backprop!() function increments the gradients its updating, rather than just setting them to some number. This lets us account for situations where the same Value contributes more than once to the final sum. In our example, x.grad will be updated twice by the backprop!() function â€“ once during the backprop!(w) call and once during the backprop!(z) call. Both of these updates will increase x.grad by 1, leaving us with our final answer of fracdwdx = 2.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Still with me? Alright, last part. We said before that backprop!() is an internal function that won't actually be called by the user. Rather, the user will call a wrapper function backward() on the final sum Value, and that function will do the full backward pass by calling backprop!() as many times as required to calculate the derivatives for all of the input Values. So now we need to write the backward() function.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"First let's take a look at the full code, and then we'll discuss what each part is doing:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backward(a::Value)\n\n    function build_topo(v::Value, visited=Value[], topo=Value[])\n        if !(v in visited)\n            push!(visited, v)\n\n            if v.op != nothing\n                for operand in v.op.args\n                    if operand isa Value\n                        build_topo(operand, visited, topo)\n                    end\n                end\n            end\n\n            push!(topo, v)\n        end\n        return topo\n    end\n    \n    topo = build_topo(a)\n\n    a.grad = 1.0\n    for node in reverse(topo)\n        backprop!(node)\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"When we call backward(a) (where a is the final result of some operations between Values) we want two things to happen. First, we want to fill up an array with a itself, and all of the other Values that were used to create a, sorted in topological order so that a Value comes after all of the dependencies used to calculate that Value â€“ meaning that a should be the last element in the array since everything else is a dependency of a. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"We can build this array with a recursive depth-first search. This is what the nested function build_topo() is doing. build_topo() returns the topologically sorted array of all the Values, with a at the end. Then we set a.grad = 1, since the derivative of a variable with respect to itself is 1. Finally, we iterate backwards through the list of Values and call backprop!() on each one to update the gradients of its operands. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"That's it! We're done! With the code we've written up to this point, we can do as many addition operations between Values as we want, then do a backward pass on the final sum to calculate its derivative with respect to all the inputs that went into it. We did it!","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Now some of you are probably thinking Wait a minute, we're not done! All we have is an addition operation for Values, and we haven't even started with Tensors yet! Ok, yeah, that's true. I guess what I mean is we're done with the difficult part â€“ the Value object structure, the logic of operation-tracking, and gradient-updating through backpropagation. Now that we're done with all that, adding more Value operations is easy. All we need to know is what the operation does, and how to calculate the derivative for it. Then we can use almost the exact same code we've already written, with only those parts changed. When we finally get to Tensors, the code will be almost exactly the same, except the operations and derivative calculations will be for matrix/vector form.","category":"page"},{"location":"under_the_hood/#Adding-some-robustness","page":"Under the Hood","title":"Adding some robustness","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, time for a short digression. Let's take another look at our Value definition:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"mutable struct Value{opType} <: Number\n    data::Float64\n    grad::Float64\n    op::opType\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"As we mentioned before, the <: Number part means that Value is a subtype of Number. This subtyping isn't strictly necessary (and in fact an earlier version of this package didn't have it), but it will make it easy for us to add some robustness to Values. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"For example, with our code so far, we can only do addition operations between Values. This is a good start, but ideally we'd also be able to do operations between Values and regular numbers, and have the output of those operations be a Value. Luckily, we can implement this with one line of code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Base.promote_rule(::Type{<:Value}, ::Type{T}) where {T<:Number} = Value","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"This line defines a promotion rule, which specifies that for operations involving a Value and a Number, the Number should be converted to a Value for the purposes of the operation, and the result should be a Value. Now we should be able to run the following code without a problem:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"test = Value(2.0) + Value(3.0) # Value + Value\nprintln(test)\n# output: Value(5.0)\n\ntest = Value(2.0) + 3.0 # Value + Number\nprintln(test)\n# output: Value(5.0)\n\ntest = 2.0 + Value(3.0) # Number + Value\nprintln(test)\n# output: Value(5.0)","category":"page"},{"location":"under_the_hood/#More-*Value*-operations","page":"Under the Hood","title":"More Value operations","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Alright, so let's add some more Value operations. We'll start with multiplication. Here's the code, for both the operation and the backward pass:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.*\nfunction *(a::Value, b::Value)\n\n    out = a.data * b.data\n    result = Value(out, 0.0, Operation(*, (a, b) )) # Value(data, grad, op)\n    return result\n\nend\n\n# backprop for multiplication operation\nfunction backprop!(val::Value{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n\n    val.op.args[1].grad += val.op.args[2].data * val.grad\n    val.op.args[2].grad += val.op.args[1].data * val.grad\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"That's it! Told you it was easy! The *(a::Value, b::Value) function is almost exactly the same as the addition function we wrote before, except that we're setting out = a.data * b.data and recording the operation as Operation(*, (a, b) ). ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The backprop!() function is also very similar to the one we wrote for addition, with just a couple small changes. First of all, we're now using where {FunType<:typeof(*), ArgTypes} in the funciton definition to specify that this is the version of backprop!() to use when the input variable was created with a multiplication operation (again, the cool thing about multiple dispatch is that we can define several versions of a function with different input types).","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The second minor difference is that we need to change the way the derivates are calculated, since we're dealing with multiplication rather than addition. For a multiplicaiton operation z = xy the derivatives of z with respect to x and y are fracdzdx = yand fracdzdy = x. The two lines inside backprop!() are just saying this in code â€“ the gradient of each operand is incremented by the value of the other operand multiplied by the val.grad (the result of the operation) to allow for the chain rule. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"With the code we've written so far, we can do things like this:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Value(2.0)\nm = Value(4.0)\nb = Value(7.0)\n\ny = m*x + b\n\nbackward(y)\n\nprintln(m.grad)\n# output: 2.0\n\nprintln(x.grad)\n# output: 4.0\n\nprintln(b.grad)\n# output: 1.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"By the way, Julia will still take care of the order of operations for us here, so we could have written y = b + m * x and gotten the same answer.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"A lot of the operations will be like the multiplication case, where we'll need to write a new backprop!() operation. However, sometimes we can find a clever way to do things that avoids this. For example, this is how we'll implement Value subtraction:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.-\n\n# negation\nfunction -(a::Value)\n\n    return a * -1\n\nend\n\n# subtraction\nfunction -(a::Value, b::Value)\n\n    return a + (-b)\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The first function -(a::Value) allows us to negate Values with a minus sign. This can be done by multiplying the Value by -1, an operation we can already do with our *(a::Value, b::Number) function. The second function -(a::Value, b::Value) allows us to do subtraction with Values by negating the second Value and then adding them together. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Pretty clever, right? This way we don't need to write a new backprop!() function for subtraction, because we've turned the subtraction operation into a combination of multiplication and addition. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Anyway, from here it's just a matter of adding more operations so that we can do more calculations with our Values. There are the operations currently supported:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Addition\nSubtraction\nMultiplication\nDivision\nExponents\ne^x\nlog()\ntanh()","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"If you've understood everything up to this point, you should be able to read all the source code for the Values and make sense of it. If there are any operations you'd like to see added, either let me know and I'll try to add them, or you can also write them yourself and submit a pull request!","category":"page"},{"location":"under_the_hood/#*Tensor*-composite-type","page":"Under the Hood","title":"Tensor composite type","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Tensors work almost exactly the same way as Values, except with a little bit of extra complications that come with dealing with vectors and matrices. But the fundamentals are basically the same. We'll track operations with our Operation objects, override several base Julia functions to work for Tensor operations, and implement the backward pass with an internal backprop!() function and a user-facing backward() function.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"The Operation object structure is the same as before:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"struct Operation{FuncType,ArgTypes}\n    op::FuncType\n    args::ArgTypes\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"And here's our definition of the Tensor object structure:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"mutable struct Tensor{opType} <: AbstractArray{Float64, 2}\n    data::Array{Float64, 2}\n    grad::Array{Float64, 2}\n    op::opType\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"As you can see, it's very similary to the Value object, except that the Tensor.data and Tensor.grad fields are arrays rather than numbers. Note also that just as Value is a subtype of Number, Tensor is a subtype of AbstractArray{Float64, 2}, a 2-dimensional array of type Float64.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"We're going to have two different contstructors for our Tensor type - one to create a Tensor from a 2D array, and another to create a Tensor from a 1D array. Here's the 2D constructor:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the Tensor constructor:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Tensor(x::Array{Float64,2}) = Tensor(x, zeros(Float64, size(x)), nothing)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, same basic idea as the Value constructor, except that we're dealing with arrays instead of numbers. Note that the constructor requires a Float64 array as input and won't accept other number types. Maybe I'll change that later to make it more robust.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Besides the 2D constructor, we also want the option to pass a 1D array and have it create either a row vector or column vector Tensor. For example, we want to be able to write this code, where we pass in a 1D array with shape (3,) and get a row vector Tensor with shape (1,3):","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = [1.0, 2.0, 3.0]\n\nprintln(size(x))\n# output: (3,)\n\nx = Tensor(x)\n\nprintln(size(x))\n# output: (1,3)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the 1D constructor that will let us do that:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function Tensor(x::Array{Float64, 1}; column_vector::Bool=false)\n\n    if column_vector\n        # column vector - size (N,1)\n        data_2D = reshape(x, (length(x), 1))\n    else\n        # DEFAULT row vector - size (1,N)\n        data_2D = reshape(x, (1,length(x)))\n    end\n\n    Tensor(data_2D, zeros(Float64, size(data_2D)), nothing) # Tensor(data, grad, op)\n    \nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Pretty simple, we just take in the 1D array and reshape it to a row vector with reshape(x, (1,length(x))) by default, or to a column vector with reshape(x, (length(x), 1)) if the user sets column_vector::Bool=true.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Just a couple more quick things, which are also similar to our Value setup. The following code lets us print out Tensors, sets the backprop!() function to be nothing in cases where a Tensor was defined by the user rather than being created in an operation, and specifies that the equality check == will return true only if the two variables actually reference the same Tensor object:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.show\nfunction show(io::IO, tensor::Tensor)\n    print(io, \"Tensor(\",tensor.data, \")\")\nend\n\nbackprop!(tensor::Tensor{Nothing}) = nothing\n\nimport Base.==\nfunction ==(a::Tensor, b::Tensor)\n    return a===b\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So that covers the necessary parts. Now we can also add a couple more things just for convenience. Here's a line of code that will allow us to call size(Tensor) to get the shape of the Tensor.data field (which should be the same as the Tensor.grad field):","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Base.size(x::Tensor) = size(x.data)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's a line of code that allows use to use standard array index notation to access elements of Tensor.data. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Base.getindex(x::Tensor, i...) = getindex(x.data, i...)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So for example, if we have a Tensor called a, we can access elements of a.data by writing a[i,j] instead of having to write a.data[i,j].","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's a similar line that allows us to set elements using standard array index notation:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Base.setindex!(x::Tensor, v, i...) = setindex!(x.data, v, i...)","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So if we want to set an element of a to 7, for example, we can just write a[i,j] = 7, rather than having to write a.data[i,j] = 7. So with those three lines added, we should now be able to run the following code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"x = Tensor([1.0 2.0; 3.0 4.0])\n\nprintln(size(x))\n# output: (2,2)\n\nx[2,2] = 5.0\n\nprintln(x[2,2])\n# output: 5.0","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Ok, now let's try defining a Tensor operation. When we were learning about how Values work we started with addition because that seemed like the easiest. But for Tensors, addition will actually be a little tough because of some shape-broadcasting we'll need to do. So we'll start with matrix multiplication, since that will be easier. Here's the code:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.*\nfunction *(a::Tensor, b::Tensor)\n\n    out = a.data * b.data\n    result = Tensor(out, zeros(Float64, size(out)), Operation(*, (a, b))) # Tensor(data, grad, op)\n    return result\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Very similar to what we were doing with Values, except that this time it's matrix multiplication. But same idea. We do the matrix multiplication with out = a.data * b.data. Then we store the resulting matrix out along with an emptry gradient size(out)) in a new Tensor called result, and record the operation as Operation(*, (a, b)). ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"And here's the backprop!() function:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(*), ArgTypes}\n\n    tensor.op.args[1].grad += tensor.grad * transpose(tensor.op.args[2].data)\n    tensor.op.args[2].grad += transpose(tensor.op.args[1].data) * tensor.grad \n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, basically the same idea as with the Value backprop!() functions. The only difficult part is that now everything has to be done for matrices, which makes the actual calculations more complicated and less intuitive. If you want, you can always work out a simple example on paper to prove to yourself that the gradient updates in this backprop!() function are actually correct. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Finally, here's the code for the full backward pass:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function backward(a::Tensor)\n\n    function build_topo(v::Tensor, visited=Tensor[], topo=Tensor[])\n        if !(v in visited)\n            push!(visited, v)\n\n            if v.op != nothing\n                for operand in v.op.args\n                    if operand isa Tensor\n                        build_topo(operand, visited, topo)\n                    end\n                end\n            end\n\n            push!(topo, v)\n        end\n        return topo\n    end\n    \n    topo = build_topo(a)\n\n    a.grad .= 1.0\n    for node in reverse(topo)\n        backprop!(node)\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Again, almost exactly the same as for the Values.","category":"page"},{"location":"under_the_hood/#More-*Tensor*-operations","page":"Under the Hood","title":"More Tensor operations","text":"","category":"section"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"For the sake of completeness, I'm going to give you the code for all of the Tensor operations needed to write a basic neural network. For now, working through the details to understand them will be left as an exercise for the reader, although I'll probably try to come back to this section to write a more complete description when I have some more free time.","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's addition:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"import Base.+\nfunction +(a::Tensor, b::Tensor)\n\n    # broadcasting happens automatically for row-vector\n    out = a.data .+ b.data\n    result = Tensor(out, zeros(Float64, size(out)), Operation(+, (a, b))) # Tensor(data, grad, op)\n    return result\n\nend\n\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(+), ArgTypes}\n\n    if size(tensor.grad) == size(tensor.op.args[1].data)\n        tensor.op.args[1].grad += ones(size(tensor.op.args[1].data)) .* tensor.grad\n    else\n        # reverse broadcast\n        tensor.op.args[1].grad += ones(size(tensor.op.args[1].grad)) .* sum(tensor.grad,dims=1)\n    end\n\n    if size(tensor.grad) == size(tensor.op.args[2].data)\n        tensor.op.args[2].grad += ones(size(tensor.op.args[2].data)) .* tensor.grad\n    else\n        # reverse broadcast\n        tensor.op.args[2].grad += ones(size(tensor.op.args[2].grad)) .* sum(tensor.grad,dims=1)\n    end\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Hint about this one: the complicated parts are related to broadcasting so that we can add add a row vector to a matrix and have it automatically be added to every row (like adding biases in a neural net).","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the ReLU activation function:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function relu(a::Tensor)\n\n    out = max.(a.data,0)\n    result = Tensor(out, zeros(Float64, size(out)), Operation(relu, (a,))) # Tensor(data, grad, op)\n    return result\n\nend\n\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(relu), ArgTypes}\n\n    tensor.op.args[1].grad += (tensor.op.args[1].data .> 0) .* tensor.grad\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Here's the combined softmax activation and cross entropy loss:","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"function softmax_crossentropy(a::Tensor,y_true::Union{Array{Int,2},Array{Float64,2}}; grad::Bool=true)\n\n    ## implementing softmax activation and cross entropy loss separately leads to very complicated gradients\n    ## but combining them makes the gradient a lot easier to deal with\n\n    ## credit to Sendex and his textbook for teaching me this part\n    ## great textbook for doing this stuff in Python, you can get it here:\n    ## https://nnfs.io/\n\n    # softmax activation\n    exp_values = exp.(a.data .- maximum(a.data, dims=2))\n    probs = exp_values ./ sum(exp_values, dims=2)\n\n    probs_clipped = clamp.(probs, 1e-7, 1 - 1e-7)\n    # deal with 0s and 1s\n\n\n    # basically just returns an array with the probability of the correct answer for each batch\n    correct_confidences = sum(probs_clipped .* y_true, dims=2)\n\n    # negative log likelihood\n    sample_losses = -log.(correct_confidences)\n\n    # loss mean\n    out = [sum(sample_losses) / length(sample_losses)]\n\n\n\n    if grad\n\n        # it's easier to do the grad calculation here because doing it seperately will involve redoing a lot of calculations\n\n        samples = size(probs, 1)\n\n        # convert from one-hot to index list\n        y_true_argmax = argmax(y_true, dims=2)\n\n        a.grad = copy(probs)\n\n        for samp_ind in 1:samples\n\n            a.grad[samp_ind, y_true_argmax[samp_ind][2]] -= 1\n            ## this syntax y_true_argmax[i][2] is just to get the column index of the true value\n\n        end\n\n        a.grad ./= samples\n\n    end\n\n    # reshape out from (1,) to (1,1) \n    out = reshape(out, (1, 1))\n\n    result = Tensor(out, zeros(Float64, size(out)), Operation(softmax_crossentropy, (a,))) # Tensor(data, grad, op)\n\n    return result\n\nend\n\n\n# softmax_crossentropy backprop is empty because gradient is easier to calculate during forward pass\nfunction backprop!(tensor::Tensor{Operation{FunType, ArgTypes}}) where {FunType<:typeof(softmax_crossentropy), ArgTypes}\n\nend","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"Note: this is the most complicated one by far, and is also the odd-one-out in that the gradient is actually calculated during the forward pass (unless told not to), which the backprop!() function just does nothing. ","category":"page"},{"location":"under_the_hood/","page":"Under the Hood","title":"Under the Hood","text":"So yeah, sorry to leave you guys with a complicated one here. I'll probably come back later and try to write a more thorough description of this one when I have some more free time in the future.","category":"page"},{"location":"#Introduction","page":"Welcome","title":"Introduction","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Hi, thanks for checking out SimpleGrad.jl! This is a gradient-tracking tool for basic machine learning applications, including neural nets. But unlike other ML packages, the primary goal of SimpleGrad is to be educational. The idea is that the source code should be easy enough to read, understand, and edit/customize, and the package should be both usable for basic applications and also a helpful for people who are learning Julia or ML (or both).","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"This documentation site also includes an Under the Hood section that explains how everything works, so that you can recreate it from scratch if you want to. My goal here was to write it like a textbook chapter, meant for people who like to understand how things work from first principles.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"For people who prefer to learn from videos, I've also made a Neural Nets from Scratch in Julia Youtube series explaining how to recreate this package from scratch and how everything works along the way.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"By the way, this project is an ongoing work-in-progress and I'm open to suggestions, criticisms, questions, and pull-requests. You can reach me by email at mikest@udel.edu.","category":"page"},{"location":"#Installation","page":"Welcome","title":"Installation","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"If everything's working properly, you should be able to install SimpleGrad right from the Julia package manager like this:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"using Pkg\nPkg.add(\"SimpleGrad\")","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Besides that, another option is to just grab the source code file and put it on your computer. Then, just direct Julia to its location in order to use it. Like this, but replace the location with its location on your computer:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"push!(LOAD_PATH, \"/Users/mikesaint-antoine/Desktop/\") \n# change this to the location of the folder where SimpleGrad.jl is on your computer\n\nusing SimpleGrad","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"In fact, even if installation through Pkg is working properly, grabbing the source code directly might be the better approach for educational purposes, so that you can easily edit it and play around with it.","category":"page"},{"location":"#Credits","page":"Welcome","title":"Credits","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Huge thanks to these people for teaching me how to do this stuff:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Andrej Karpathy for his Micrograd tutorial\nHarrison Kinsley (Sentdex on Youtube) for his Neural Net From Scratch in Python textbook\nu/Bob_Dieter for his reddit comments","category":"page"},{"location":"tutorials/linear_regression/#Linear-Regression-Example","page":"Linear Regression","title":"Linear Regression Example","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's try using Values to fit a line to data, using gradient descent.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"First, we'll make up some fake data of two things with an assumed-linear relationship: height and basketball-skills. We'll define the true slope and intercept of the relationship as m=2 and b=10.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"using Random\nRandom.seed!(1234)\n# seeding the random number generator for reproducibility\n\nheights = Float64[]\nfor count in 1:79\n    push!(heights, rand(40:90))\nend\n\n\n# TRUE PARAMS y = m*x + b\nm = 2\nb = 10\n\n\n\nskills = Float64[]\n\nfor height in heights\n    skill = m * height + b + randn() * 7.0\n    push!(skills, skill)\nend","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Just for fun, I'll add myself to this dataset. I'm 72 inches tall, and extremely bad at basketball lol","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"push!(heights, 72)\npush!(skills, 75)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Now let's plot the data just to take a look at it:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"using Plots\nscatter(heights, skills, legend=false, markersize=3, color=:black, xlabel=\"Height (inches)\", ylabel=\"Basketball Skills\",dpi=300)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: scatter_plot)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Ok, now let's see if we can use Values to fit a line to this data. First, we'll cast all of the height and skill measurements we have to Values","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"heights = [Value(item) for item in heights]\nskills = [Value(item) for item in skills]","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Next we'll set two hyper-parameters. We'll set the learning rate lr = 0.000002 and the number of iterations of the fitting to runs = 100000.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"lr = 0.000002\nruns = 100000","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Now we'll define our initial guesses:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"m_guess = Value(0)\nb_guess = Value(0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Now we can actually fit the parameters to the data. We'll do this with runs = 100000 iterations, updating the parameters each time. For each iteration, we start by reseting the m_guess and b_guess gradients to 0. Then, we set global loss = Value(0). We then iterate through our heights array, and calculate the predicted skills using our current parameters as: skill_pred = heights[i] * m_guess + b_guess. We then compare the predicted skill level to the actual skill level for that height and compute the squared error between them: loss_to_add = (skill_pred - skills[i])^2. Then, we add that amount to the total loss: global loss += loss_to_add. All of these calculations are valid operations for our Values, so at this point we can call backward(loss) to calculate the gradients of the parameters. We then update the parameters in the opposite direction of the gradients, scaled by the learning rate. Here's the entire block of code:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"for run in 1:runs\n\n    # reset the grads to 0 for each iteration\n    m_guess.grad = 0\n    b_guess.grad = 0\n\n    # set loss to 0\n    global loss = Value(0)\n\n    for i in 1:length(heights)\n\n        # predicted skill level, using our parameters\n        skill_pred = heights[i] * m_guess + b_guess \n\n        # squared error compared to actual skill level\n        loss_to_add = (skill_pred - skills[i])^2\n\n        global loss += loss_to_add\n    end\n\n    # backward pass to calculate the gradients\n    backward(loss)\n\n    # updating parameters\n    m_guess.data -= m_guess.grad * lr\n    b_guess.data -= b_guess.grad * lr\n\nend","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's see where our guesses for m and b are at now.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"println(m_guess)\n# output: Value(1.9896292719430697)\n\nprintln(b_guess)\n# output: Value(9.640345678405836)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Pretty close to the real values that we originally used to make the data! We can also plot the fit line with these m_guess and b_guess parameters:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"heights_data = [item.data for item in heights] # remember heights is full of Values, so need to do this to get the numbers\n\nx_line = minimum(heights_data):maximum(heights_data)\ny_line = m_guess.data * x_line .+ b_guess.data\nplot!(x_line, y_line, linewidth=2, color=:blue)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: scatter_plot)","category":"page"},{"location":"tutorials/mnist/#MNIST-Example","page":"MNIST","title":"MNIST Example","text":"","category":"section"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"In this section, we'll use Tensors for a real neural net example â€“ solving the MNIST image classification problem. The idea in this problem is to use a neural net to \"read\" images of hand-drawn numbers, from 0-9, and correctly identify which number each one is.","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"However, we're gonna gonna be a bit lazy and work with pre-processed data in CSV format, rather than actually reading in the images ourselves. You can download the MNIST data in CSV format here.","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"First, we'll read in the training data. We want to store the features (pixel values from the images) in an array called X, and store the labels (the correct answers for which number each one is) in an array called y. Lastly, we scale the features by diving them by 255, to get them between 0 and 1. Here's the code:","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"X = []\ny = []\nglobal first_row = true\nopen(\"mnist_data/mnist_train.csv\", \"r\") do file\n    for line in eachline(file)\n\n        if first_row  # skip the first row\n            global first_row = false\n            continue\n        end\n\n        # split the line by comma and strip whitespace\n        row = parse.(Float64, strip.(split(line, ',')))\n\n        push!(y, row[1])\n        push!(X, row[2:length(row)])\n    end\nend\n\nX= hcat(X...)'\nX = X / 255.0","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Next, we do the same thing for the testing data, except we save the features and labels in arrays called X_test and y_test. Here's the code:","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"X_test = []\ny_test = []\nglobal first_row = true\nopen(\"mnist_data/mnist_test.csv\", \"r\") do file\n    for line in eachline(file)\n\n        if first_row  # skip the first row\n            global first_row = false\n            continue\n        end\n\n        # split the line by comma and strip whitespace\n        row = parse.(Float64, strip.(split(line, ',')))\n\n        push!(y_test, row[1])\n        push!(X_test, row[2:length(row)])\n    end\nend\n\nX_test = hcat(X_test...)'\nX_test = X_test / 255.0","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Next, we define the model. We want our model to take in 784 inputs per sample (the size of the flattened 28x28 pixel images). Our inner layer will have 128 neurons, and our final layer will have 10 neurons (corresponding to the 10 possible digits that the image could be). So, we'll need to make sure we shape our Tensors accordingly. Here's the code:","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"using SimpleGrad\nusing Random\nRandom.seed!(1234)\n# seeding the random number generator for reproducibility\n\nweights1 = Tensor(0.01 * rand(784, 128))\nweights2 = Tensor(0.01 * rand(128, 10))\n\nbiases1 = Tensor(zeros(128))\nbiases2 = Tensor(zeros(10))\n\n\nbatch_size = 100\nnum_classes = 10  # total number of classes\nlr = 0.1\nepochs = 2","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Now, we train the model:","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"global run = 1\nfor epoch in 1:epochs\n\n    for i in 1:batch_size:size(X,1)\n\n\n        ## get current batch\n        batch_X = X[i:i+batch_size-1, :]\n        batch_X = Tensor(batch_X)\n        batch_y = y[i:i+batch_size-1]\n\n\n\n        ## convert batch_y to one-hot\n        batch_y_one_hot = zeros(batch_size,num_classes)\n        for batch_ind in 1:batch_size\n            batch_y_one_hot[batch_ind,Int.(batch_y)[batch_ind]+1] = 1\n        end\n\n\n\n        ## zero grads\n        weights1.grad .= 0\n        weights2.grad .= 0\n        biases1.grad .= 0\n        biases2.grad .= 0\n\n\n\n        ## forward pass\n        layer1_out = relu(batch_X * weights1 + biases1)\n        layer2_out = layer1_out * weights2 + biases2\n        loss = softmax_crossentropy(layer2_out,batch_y_one_hot)\n\n\n\n        ## backward pass\n        backward(loss)\n\n\n        ## update params\n        weights1.data -= weights1.grad .* lr\n        weights2.data -= weights2.grad .* lr\n        biases1.data -= biases1.grad .* lr\n        biases2.data -= biases2.grad .* lr\n\n\n        if run % 100 == 0\n            println(\"Epoch: $epoch, run: $run, loss: $(round(loss.data[1], digits=3))\")\n        end\n        \n        global run += 1\n\n    end\nend\n\n# output:\n# Epoch: 1, run: 100, loss: 1.145\n# Epoch: 1, run: 200, loss: 0.55\n# Epoch: 1, run: 300, loss: 0.677\n# Epoch: 1, run: 400, loss: 0.491\n# Epoch: 1, run: 500, loss: 0.331\n# Epoch: 1, run: 600, loss: 0.388\n# Epoch: 2, run: 700, loss: 0.225\n# Epoch: 2, run: 800, loss: 0.299\n# Epoch: 2, run: 900, loss: 0.502\n# Epoch: 2, run: 1000, loss: 0.355\n# Epoch: 2, run: 1100, loss: 0.248\n# Epoch: 2, run: 1200, loss: 0.337","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"Finally, we check out performance on the testing set:","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"global correct = 0\nglobal total = 0\nfor i in 1:length(y_test)\n    X_in = X_test[i:i,:] ## need to keep this (1,784), not (784,)\n    X_in = Tensor(X_in)\n    y_true = y_test[i]\n\n    layer1_out = relu(X_in * weights1 + biases1)\n    layer2_out = layer1_out * weights2 + biases2\n\n\n    pred_argmax = argmax(layer2_out.data, dims=2)[1][2]\n\n    if pred_argmax-1 == y_true\n        global correct +=1\n    end\n    global total += 1\n\nend\n\nprintln(correct/total)\n# output: 0.9187","category":"page"},{"location":"tutorials/mnist/","page":"MNIST","title":"MNIST","text":"91.87% accuracy on the testing set. Not bad!","category":"page"}]
}
